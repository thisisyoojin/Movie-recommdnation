{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System\n",
    "\n",
    "### Part 3: Evaluation\n",
    "\n",
    "For the evaluation, I used stratified cross-validation and Top-n accuracy. In the group consisting of 1 reviewed-item of test data and randomly sampled 100 non-reviewed items, the model produced a ranked list of recommended items. The model evaluator checked whether the interacted item is among the top N items (hit) in the ranked list of 101 recommendations for a user. It computes the Top-N accuracy and Recall for each users and the results will be aggregated for evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_RANDOM_SAMPLE_NON_REVIEWED_ITEMS = 100\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \n",
    "    def __init__(self, rating_test_df):\n",
    "        self.rating_test_df = rating_test_df\n",
    "    \n",
    "    def get_not_reviewed_sample(self, person_id, sample_size, seed=42):\n",
    "        reviewed_items = get_items_reviewed(person_id, self.rating_test_df)\n",
    "        all_items = np.asarray(range(0,10000))\n",
    "        non_reviewed_items = set(all_items) - set(reviewed_items)\n",
    "\n",
    "        random.seed(seed)\n",
    "        non_reviewed_samples = random.sample(non_reviewed_items, sample_size)\n",
    "        return set(non_reviewed_samples)\n",
    "\n",
    "    #returning whether if the movie is in topn list and the index\n",
    "    def _verify_hit_top_n(self, movie_id, recommended_items, topn):        \n",
    "        try:\n",
    "            index = next(i for i in range(len(recommended_items)) if recommended_items[i] == movie_id)\n",
    "        except:\n",
    "            index = -1\n",
    "        hit = int(index in range(0, topn))\n",
    "        return hit, index\n",
    "\n",
    "        \n",
    "    def evaluate_model_for_user(self, model, person_id):\n",
    "        \n",
    "        #Getting the items in test set\n",
    "        person_testset = rating_indexed_test.loc[person_id]\n",
    "        movies_test = person_testset['movie_id'].values\n",
    "        movies_cnt_test = len(movies_test)\n",
    "\n",
    "        #Getting a ranked recommendation list from a model for a given user\n",
    "        person_recs_df = model.recommend_items(person_id, topn=1000000)\n",
    "\n",
    "        hits_at_5_count = 0\n",
    "        hits_at_10_count = 0\n",
    "        \n",
    "        #For each item the user has interacted in test set\n",
    "        for item_id in movies_test:\n",
    "            #Getting a random sample (100) items the user has not interacted \n",
    "            #(to represent items that are assumed to be no relevant to the user)\n",
    "            non_reviewed_sample = self.get_not_reviewed_sample(person_id, sample_size=EVAL_RANDOM_SAMPLE_NON_REVIEWED_ITEMS)\n",
    "\n",
    "            #Combining the current interacted item with the 100 random items\n",
    "            items_to_filter_recs = non_reviewed_sample.union(set([item_id]))\n",
    "          \n",
    "\n",
    "            #Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n",
    "            valid_recs_df = person_recs_df[person_recs_df['movie_id'].isin(items_to_filter_recs)]\n",
    "            valid_recs = valid_recs_df['movie_id'].values\n",
    "            \n",
    "            \n",
    "            #Verifying if the current interacted item is among the Top-N recommended items\n",
    "            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n",
    "            hits_at_5_count += hit_at_5\n",
    "            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n",
    "            hits_at_10_count += hit_at_10\n",
    "            \n",
    "\n",
    "        #Recall is the rate of the interacted items that are ranked among the Top-N recommended items, \n",
    "        #when mixed with a set of non-relevant items\n",
    "        recall_at_5 = hits_at_5_count / float(movies_cnt_test)\n",
    "        recall_at_10 = hits_at_10_count / float(movies_cnt_test)\n",
    "\n",
    "        person_metrics = {'hits@5_count':hits_at_5_count, \n",
    "                          'hits@10_count':hits_at_10_count, \n",
    "                          'reviewed_count': movies_cnt_test,\n",
    "                          'recall@5': recall_at_5,\n",
    "                          'recall@10': recall_at_10}\n",
    "        return person_metrics\n",
    "\n",
    "    \n",
    "    def evaluate_model(self, model):\n",
    "\n",
    "        people_metrics = []\n",
    "        for idx, person_id in enumerate(list(rating_indexed_test.index.unique().values)):\n",
    "            person_metrics = self.evaluate_model_for_user(model, person_id)  \n",
    "            person_metrics['_person_id'] = person_id\n",
    "            people_metrics.append(person_metrics)\n",
    "        \n",
    "       \n",
    "\n",
    "        detailed_results_df = pd.DataFrame(people_metrics) \\\n",
    "                            .sort_values('reviewed_count', ascending=False)\n",
    "        \n",
    "        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['reviewed_count'].sum())\n",
    "        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['reviewed_count'].sum())\n",
    "        \n",
    "        global_metrics = {'modelName': model.get_model_name(),\n",
    "                          'recall@5': global_recall_at_5,\n",
    "                          'recall@10': global_recall_at_10}  \n",
    "        \n",
    "        return global_metrics, detailed_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "By using the model evaluator, I got recall scores from each models and saved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Collaborative Filtering',\n",
       " 'recall@5': 0.35171620558145167,\n",
       " 'recall@10': 0.5193262304013561}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval = ModelEvaluator(rating_test_df)\n",
    "cf_result, cf_result_df = model_eval.evaluate_model(cf_rec)\n",
    "cf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Content-Based Filtering',\n",
       " 'recall@5': 0.05691930504267813,\n",
       " 'recall@10': 0.1111447424178219}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbf_result, cbf_result_df = model_eval.evaluate_model(cbf_rec)\n",
    "cbf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Hybrid Model',\n",
       " 'recall@5': 0.018130637447787395,\n",
       " 'recall@10': 0.018130637447787395}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hb_rec_result, hb_rec_result_df = model_eval.evaluate_model(hb_rec)\n",
    "hb_rec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelresult.pkl', 'wb') as fout:\n",
    "    pickle.dump({\n",
    "        'cf': [cf_result, cf_result_df],\n",
    "        'cbf': [cbf_result, cbf_result_df],\n",
    "        'hyb': [hb_rec_result, hb_rec_result_df]\n",
    "    }, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
